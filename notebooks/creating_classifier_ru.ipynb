{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отзывы для обучения уже загружены скриптом в SQLite3 бд.\n",
    "\n",
    "Часть данных была вручную размечена для улучшения качества. Эти датасеты не включены в репозиторий, однако я готов ими поделиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import html\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  Плюсы: Хорошая камера, получаются четкие снимк...      1\n",
       "1  Плюсы: Это мой четвертый Xiaomi, один лучше др...      1\n",
       "2  Плюсы: безрамочный, цвета оч. сочные, камера 6...      1\n",
       "3  Плюсы: Мощный процессор, 6 Gb памяти, отличная...      1\n",
       "4  Плюсы: Яркий экран, отличное качество фото. Ми...      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Плюсы: Хорошая камера, получаются четкие снимк...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Плюсы: Это мой четвертый Xiaomi, один лучше др...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Плюсы: безрамочный, цвета оч. сочные, камера 6...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Плюсы: Мощный процессор, 6 Gb памяти, отличная...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Плюсы: Яркий экран, отличное качество фото. Ми...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"./train_data.csv\", usecols=[\"text\", \"label\"])\n",
    "train_data[\"label\"] = train_data[\"label\"].map({\"pos\":1,\"neg\":0})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Плюсы: Хорошая камера, получаются четкие снимки в режиме 64мп Производительность Красивый Хороший экран Держит заряд целый день и еще остаётся процентов 30, если не играть в игры Соотношение цена/качество Присутствует модуль NFS . Минусы: Сканер отпечатка пальцев находиться рядом с камерами Вырез капелька Маркий и очень скользкий Убрали возможность записывать звонки, но это не только у этой версии телефона. Впечатления: Очень хороший телефон, первый из линейки Redmi у которого есть NFC, многие бояться процессора MediaTek, но телефон очень производительный, тянет игры на максималках и не сильно греется. Камеры бомба, присутствует даже макрообъектив. Без чехла лучше не носить, скользкий и легко уронить. В общем достойная модель, учитывая что на алике можно заказать за 14к .'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "train_data.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Подготовка текста"
   ]
  },
  {
   "source": [
    "Попробую аугментировать мой набор данных. Специфика такая, что в отзыв могут написать как хорошее, так и плохое (для этого есть отдельные секции). Две идеи\n",
    "\n",
    "1. Добавить отзывы, состоящие только из содержимого секций \"Плюсы\" и \"Минусы\" с соответствующими лейблами\n",
    "2. Добавить отзывы, в которых, в зависимости от известного лейбла, убрано содержимое противоположной секции"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_chunks = []\n",
    "for _, row in train_data.iterrows():\n",
    "    try:\n",
    "        _positive, _remaining = row.text.split(\"Минусы: \")\n",
    "        _negative, _remaining = _remaining.split(\"Впечатления: \")\n",
    "        separated_chunks.append({\"positive\": _positive, \"negative\": _negative, \"other\": _remaining, \"label\": row.label})\n",
    "    except ValueError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10151"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "len(separated_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data_synthetic_labels = []\n",
    "for chunk in separated_chunks:\n",
    "    aug_data_synthetic_labels.append({\"text\": chunk[\"positive\"], \"label\": 1})\n",
    "    aug_data_synthetic_labels.append({\"text\": chunk[\"negative\"], \"label\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data_irrelevant_separated = []\n",
    "for chunk in separated_chunks:\n",
    "    aug_data_synthetic_labels.append({\n",
    "        \"text\": chunk[\"positive\"] + \" \" + chunk[\"other\"] if chunk[\"label\"] == 1 else chunk[\"negative\"] + \" \" + chunk[\"other\"], \n",
    "        \"label\": chunk[\"label\"]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.append(pd.DataFrame(aug_data_synthetic_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.append(pd.DataFrame(aug_data_irrelevant_separated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что касается предобработки, то я буду использовать два варианта: довольно стерильные наборы слов и +- оригинальные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class RoughPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self, data, y=None):\n",
    "        return list(map(self.normalize_text_re, map(self.normalize_text, data)))\n",
    "    \n",
    "    def normalize_text(self, text):\n",
    "        _t = html.unescape(text)\n",
    "        _t = _t.replace(\"Плюсы: \",\". \")\n",
    "        _t = _t.replace(\"Минусы: \",\". \")\n",
    "        _t = _t.replace(\"Впечатления: \",\". \")\n",
    "        _t = _t.replace(\"<p>\",\" \")\n",
    "        _t = _t.replace(\"</p>\", \" \")\n",
    "        _t = _t.replace(\"\\n\", \" \")\n",
    "        _t = _t.replace(\"\\r\", \" \")\n",
    "        _t = _t.replace(\"\\t\", \" \")\n",
    "        _t = _t.replace('\"', \" \")\n",
    "        _t = _t.lower()\n",
    "        return _t.strip()\n",
    "    \n",
    "    def normalize_text_re(self, text):\n",
    "        _t = text\n",
    "        _t = re.sub(r\"[\\s.,\\-\\+><;:!?()]\", \" \", _t)\n",
    "        _t = re.sub(r\"\\s+\", \" \", _t)\n",
    "        return _t.strip()\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return self.fit_transform(data)\n",
    "    \n",
    "    def transform(self, data, y=None):\n",
    "        return self.fit_transform(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy препроцессор, чтоб проще пайплайн строить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self, data, y=None):\n",
    "        return data\n",
    "    \n",
    "    def transform(self, data, y=None):\n",
    "        return data\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я хочу воспользоваться библиотекой gensim, потому что я уже использовал их эмбеддинги, и они отлично показали себя в классификации, даже на плохо обработанном датасете.\n",
    "\n",
    "Чтобы потом удобно запаковать это в sklearn pipeline, я реализую свой класс по образу `TfIdfVectorizer` из `sklearn.feature_extraction`\n",
    "\n",
    "Также я попробую TfIdfVectorizer и CountVectorizer сам по себе для сравнения какой лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import doc2vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "\n",
    "class GensimVectorizer(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        X = self.preprocess(raw_documents)\n",
    "        # print(\"Creating model...\")\n",
    "        model = gensim.models.doc2vec.Doc2Vec(\n",
    "            vector_size=100, \n",
    "            min_count=10,\n",
    "            epochs=40\n",
    "        )\n",
    "        # print(\"Building vocab...\")\n",
    "        model.build_vocab(X)\n",
    "        # print(\"Training doc2vec...\")\n",
    "        model.train(X, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_documents, y=None):\n",
    "        X = self.preprocess(raw_documents)\n",
    "        # print(\"Iinferring vectors...\")\n",
    "        vectorized_texts = []\n",
    "        for doc_id, _ in enumerate(tqdm(X, desc=\"Inferring vectors: \")):\n",
    "            inferred_vector = self.model.infer_vector(X[doc_id].words)\n",
    "            vectorized_texts.append(inferred_vector)\n",
    "\n",
    "        return vectorized_texts\n",
    "\n",
    "    def preprocess(self, raw_documents):\n",
    "        # print(\"Tokenization...\")\n",
    "        processed_texts = []\n",
    "        for idx, text in enumerate(tqdm(raw_documents, desc=\"Tokenization: \")):\n",
    "            processed_texts.append(doc2vec.TaggedDocument(simple_preprocess(text), [idx]))\n",
    "        return processed_texts\n",
    "\n",
    "\n",
    "    def fit_transform(self, texts, y=None) -> np.ndarray:\n",
    "        self.fit(texts)\n",
    "        X = self.transform(texts)\n",
    "        \n",
    "        return np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение модели "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, которые я буду рассматривать:\n",
    "+ LogisticRegression\n",
    "+ GradientBoosting\n",
    "+ LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[\"text\"]\n",
    "# y = data.label.map({\"pos\": 1, \"neg\": 0})\n",
    "y = train_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "_model = make_pipeline(DummyPreprocessor(), CountVectorizer(ngram_range=(1,2)), LinearSVC(max_iter=4000, class_weight=\"balanced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dummypreprocessor',\n",
       "                 <__main__.DummyPreprocessor object at 0x7fe9637a34f0>),\n",
       "                ('countvectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 2), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('linearsvc',\n",
       "                 LinearSVC(C=1.0, class_weight='balanced', dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=4000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.928684  , 0.98729314, 0.99931041, 0.99980296])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "cross_val_score(_model, X, y, n_jobs=6, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Улучшение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшить модель можно несколькими путями:\n",
    "+ 1. Попробовать разный препроцессинг текста\n",
    "+ 2. Подобрать параметры модели векторизации\n",
    "+ 3. Подобрать параметры классификатора\n",
    "+ 4. Попробовать другие классификаторы\n",
    "+ 5. Сбалансировать классы для обучения\n",
    "+ 0. Делать перебор не по сетке, а более \"разумными\" методами\n",
    "\n",
    "Я сделаю только пп. 1, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_ = Pipeline([\n",
    "    (\"preprocessor\", RoughPreprocessor()),\n",
    "    (\"vec\", GensimVectorizer()),\n",
    "    (\"clf\", GradientBoostingClassifier())\n",
    "])\n",
    "# я разбил на несколько словарей потому что \n",
    "# параметры не унифицированы между классами моделей\n",
    "linear_classifiers = {\n",
    "    \"clf\":[LogisticRegression(), LinearSVC()],\n",
    "    \"clf__class_weight\":[\"balanced\"],\n",
    "    \"clf__dual\": [True, False]\n",
    "}\n",
    "forest_classifiers = {\n",
    "    \"clf\":[GradientBoostingClassifier(), RandomForestClassifier()],\n",
    "    \"clf__n_estimators\":[100,200, 500],\n",
    "    \"clf__max_depth\": [3,4,5,10,20],\n",
    "}\n",
    "preprocessors = {\n",
    "    \"preprocessor\":[RoughPreprocessor(), DummyPreprocessor()],\n",
    "}\n",
    "common_vectorizers = {\n",
    "    \"vec\": [CountVectorizer(), TfidfVectorizer()],\n",
    "    \"vec__ngram_range\": [(1,1), (1,2), (1,3), (1,4), (1,5)],\n",
    "    \"vec__min_df\": [1,2,3,4,10],\n",
    "    \"vec__max_features\": [None, 200, 500, 1000]\n",
    "}\n",
    "embedding_vectorizers = {\n",
    "    \"vec\": [GensimVectorizer()]\n",
    "}\n",
    "\n",
    "param_grid = [\n",
    "{\n",
    "    # Plain Vectorizers + Linear Models\n",
    "    **preprocessors,\n",
    "    **common_vectorizers,\n",
    "    **linear_classifiers    \n",
    "}, \n",
    "# {\n",
    "#     # Gensim Embeddings + Linear Models\n",
    "#     **preprocessors,\n",
    "#     **embedding_vectorizers,\n",
    "#     **linear_classifiers,\n",
    "# }, \n",
    "# {\n",
    "#     # Gensim Embeddings + GradBoost Models\n",
    "#     **preprocessors,\n",
    "#     **embedding_vectorizers,\n",
    "#     **forest_classifiers,\n",
    "# }, \n",
    "{\n",
    "    # Plain Vectorizers + GradBoost Models\n",
    "    **preprocessors,\n",
    "    **common_vectorizers,\n",
    "    **forest_classifiers,\n",
    "}\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ~ несколько недель на 6 ядрах Ryzen5\n",
    "gscv_ = GridSearchCV(\n",
    "    estimator=pipeline_,\n",
    "    param_grid=param_grid, \n",
    "    scoring=\"accuracy\", \n",
    "    n_jobs=6, refit=True, cv=3, verbose=2)\n",
    "gscv_.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8680074382025134\n",
      "{'clf': LinearSVC(class_weight='balanced'), 'clf__class_weight': 'balanced', 'clf__dual': True, 'preprocessor': <__main__.DummyPreprocessor object at 0x7f82589ecd60>, 'vec': CountVectorizer(min_df=2, ngram_range=(1, 4)), 'vec__min_df': 2, 'vec__ngram_range': (1, 4)}\n"
     ]
    }
   ],
   "source": [
    "print(gscv_.best_score_)\n",
    "print(gscv_.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший набор параметров:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = gscv_.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = _model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Инференс и подготовка сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это для импорта предоставленного файла\n",
    "import bs4\n",
    "test = []\n",
    "with open(\"test.csv\") as tfile:\n",
    "    sp = bs4.BeautifulSoup(tfile)\n",
    "    revs = sp.findAll(\"review\")\n",
    "    for r in revs:\n",
    "        test.append(r.text)\n",
    "\n",
    "# pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а это мой размеченный тестовый файл\n",
    "test = pd.read_csv(\"./test_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"prediction\"] = model_final.predict(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.59"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "accuracy_score(test[\"y\"], test[\"prediction\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# моя разметка тестового файла не очень точная.\n",
    "accuracy_score(test[\"y\"], test[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.copy()\n",
    "submission[\"y\"] = test.prediction.map({1: \"pos\", 0: \"neg\"})\n",
    "submission.to_csv(\"./submission.csv\", columns=[\"y\"], index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Упаковка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В финальный пайп я пакую предобученную на большой выборке модель векторизации \n",
    "# и классификатор, обученный на выборке после ресемплинга\n",
    "final_pipeline = model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../SentimentModelRU.pkl\", \"wb\") as fout:\n",
    "    dill.dump(final_pipeline, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../SentimentModelRU.pkl\", \"rb\") as fin:\n",
    "    v = dill.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}