{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отзывы для обучения уже загружены скриптом в SQLite3 бд.\n",
    "\n",
    "Часть данных была вручную размечена для улучшения качества. Эти датасеты не включены в репозиторий, однако я готов ими поделиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import html\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  Плюсы: Хорошая камера, получаются четкие снимк...      1\n",
       "1  Плюсы: Это мой четвертый Xiaomi, один лучше др...      1\n",
       "2  Плюсы: безрамочный, цвета оч. сочные, камера 6...      1\n",
       "3  Плюсы: Мощный процессор, 6 Gb памяти, отличная...      1\n",
       "4  Плюсы: Яркий экран, отличное качество фото. Ми...      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Плюсы: Хорошая камера, получаются четкие снимк...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Плюсы: Это мой четвертый Xiaomi, один лучше др...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Плюсы: безрамочный, цвета оч. сочные, камера 6...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Плюсы: Мощный процессор, 6 Gb памяти, отличная...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Плюсы: Яркий экран, отличное качество фото. Ми...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"./train_data.csv\", usecols=[\"text\", \"label\"])\n",
    "train_data[\"label\"] = train_data[\"label\"].map({\"pos\":1,\"neg\":0})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Плюсы: Хорошая камера, получаются четкие снимки в режиме 64мп Производительность Красивый Хороший экран Держит заряд целый день и еще остаётся процентов 30, если не играть в игры Соотношение цена/качество Присутствует модуль NFS . Минусы: Сканер отпечатка пальцев находиться рядом с камерами Вырез капелька Маркий и очень скользкий Убрали возможность записывать звонки, но это не только у этой версии телефона. Впечатления: Очень хороший телефон, первый из линейки Redmi у которого есть NFC, многие бояться процессора MediaTek, но телефон очень производительный, тянет игры на максималках и не сильно греется. Камеры бомба, присутствует даже макрообъектив. Без чехла лучше не носить, скользкий и легко уронить. В общем достойная модель, учитывая что на алике можно заказать за 14к .'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_data.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Подготовка текста"
   ]
  },
  {
   "source": [
    "Попробую аугментировать мой набор данных. Специфика такая, что в отзыв могут написать как хорошее, так и плохое (для этого есть отдельные секции). Две идеи\n",
    "\n",
    "1. Добавить отзывы, состоящие только из содержимого секций \"Плюсы\" и \"Минусы\" с соответствующими лейблами\n",
    "2. Добавить отзывы, в которых, в зависимости от известного лейбла, убрано содержимое противоположной секции"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_chunks = []\n",
    "for _, row in train_data.iterrows():\n",
    "    try:\n",
    "        _positive, _remaining = row.text.split(\"Минусы: \")\n",
    "        _negative, _remaining = _remaining.split(\"Впечатления: \")\n",
    "        _positive = _positive.replace(\"Плюсы: \", \"\")\n",
    "        separated_chunks.append({\"positive\": _positive, \"negative\": _negative, \"other\": _remaining, \"label\": row.label})\n",
    "    except ValueError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10151"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(separated_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'positive': '1) Экран. 2) Корпус из металла. ',\n",
       " 'negative': '1) Самый жирный - система IOS (как в тюрьме или в армии) 2) Цена высокая 3) Скользкий 4) За такую цену качество должно превосходить все смартфоны, но увы - наоборот. . ',\n",
       " 'other': 'Впечатлений масса, но все они не в пользу этого смартфона (ипхона).',\n",
       " 'label': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "separated_chunks[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data_synthetic_labels = []\n",
    "for chunk in separated_chunks:\n",
    "    aug_data_synthetic_labels.append({\"text\": chunk[\"positive\"], \"label\": 1})\n",
    "    aug_data_synthetic_labels.append({\"text\": chunk[\"negative\"], \"label\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data_irrelevant_separated = []\n",
    "for chunk in separated_chunks:\n",
    "    aug_data_synthetic_labels.append({\n",
    "        \"text\": chunk[\"positive\"] + \" \" + chunk[\"other\"] if chunk[\"label\"] == 1 else chunk[\"negative\"] + \" \" + chunk[\"other\"], \n",
    "        \"label\": chunk[\"label\"]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data_feedback_only = []\n",
    "for chunk in separated_chunks:\n",
    "    aug_data_feedback_only.append({\n",
    "        \"text\": chunk[\"other\"], \n",
    "        \"label\": chunk[\"label\"]\n",
    "        })\n",
    "train_data = pd.DataFrame(aug_data_feedback_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.append(pd.DataFrame(aug_data_synthetic_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.append(pd.DataFrame(aug_data_irrelevant_separated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "randomset = []"
   ]
  },
  {
   "source": [
    "rn = random.randint(0, 10151)\n",
    "while (rn in randomset) and (len(train_data) != len(randomset)):\n",
    "    rn = random.randint(0, 10151)\n",
    "randomset.append(rn)\n",
    "print(f\"проверено: {len(randomset)}    осталось {len(train_data) - len(randomset)}\")\n",
    "print(f\"rn: {rn}\")\n",
    "print(train_data.iloc[rn].label)\n",
    "print(train_data.iloc[rn].text)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5280,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "проверено: 4203    осталось 5948\nrn: 8653\n1\nЗа свои деньги очень достойный аппарат, забыл про доп. зарядные устройства, зарядки хватает на пару дней..\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5251,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.at[rn, 'label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.at[rn, 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5057,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn = _rn"
   ]
  },
  {
   "source": [
    "_rn = randomset[-2]\n",
    "print(train_data.iloc[_rn].label)\n",
    "print(train_data.iloc[_rn].text)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5056,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\nВ эксплуатации почти 1 год. Аппарат не плохой. Работает стабильно. Батарея держит хорошо. До этого сидел на яблоке. Если сравнивать, то система iOS это премиум, андроид это эконом. От яблока отказался - за долбали обновления и умышленные действия яблока направленные на покупку новых аппаратов . .\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5281,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_data_other.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5282,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "text     В целом модель то не плохая, но с каждым обнов...\n",
       "label                                                    0\n",
       "Name: 1261, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 5282
    }
   ],
   "source": [
    "train_data.iloc[1261]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что касается предобработки, то я буду использовать два варианта: довольно стерильные наборы слов и +- оригинальные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class RoughPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self, data, y=None):\n",
    "        return list(map(self.normalize_text_re, map(self.normalize_text, data)))\n",
    "    \n",
    "    def normalize_text(self, text):\n",
    "        _t = html.unescape(text)\n",
    "        _t = _t.replace(\"Плюсы: \",\". \")\n",
    "        _t = _t.replace(\"Минусы: \",\". \")\n",
    "        _t = _t.replace(\"Впечатления: \",\". \")\n",
    "        _t = _t.replace(\"<p>\",\" \")\n",
    "        _t = _t.replace(\"</p>\", \" \")\n",
    "        _t = _t.replace(\"\\n\", \" \")\n",
    "        _t = _t.replace(\"\\r\", \" \")\n",
    "        _t = _t.replace(\"\\t\", \" \")\n",
    "        _t = _t.replace('\"', \" \")\n",
    "        _t = _t.lower()\n",
    "        return _t.strip()\n",
    "    \n",
    "    def normalize_text_re(self, text):\n",
    "        _t = text\n",
    "        _t = re.sub(r\"[\\s.,\\-\\+><;:!?()]\", \" \", _t)\n",
    "        _t = re.sub(r\"\\s+\", \" \", _t)\n",
    "        return _t.strip()\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return self.fit_transform(data)\n",
    "    \n",
    "    def transform(self, data, y=None):\n",
    "        return self.fit_transform(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy препроцессор, чтоб проще пайплайн строить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self, data, y=None):\n",
    "        return data\n",
    "    \n",
    "    def transform(self, data, y=None):\n",
    "        return data\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я хочу воспользоваться библиотекой gensim, потому что я уже использовал их эмбеддинги, и они отлично показали себя в классификации, даже на плохо обработанном датасете.\n",
    "\n",
    "Чтобы потом удобно запаковать это в sklearn pipeline, я реализую свой класс по образу `TfIdfVectorizer` из `sklearn.feature_extraction`\n",
    "\n",
    "Также я попробую TfIdfVectorizer и CountVectorizer сам по себе для сравнения какой лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import doc2vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "\n",
    "class GensimVectorizer(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        X = self.preprocess(raw_documents)\n",
    "        # print(\"Creating model...\")\n",
    "        model = gensim.models.doc2vec.Doc2Vec(\n",
    "            vector_size=100, \n",
    "            min_count=10,\n",
    "            epochs=40\n",
    "        )\n",
    "        # print(\"Building vocab...\")\n",
    "        model.build_vocab(X)\n",
    "        # print(\"Training doc2vec...\")\n",
    "        model.train(X, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_documents, y=None):\n",
    "        X = self.preprocess(raw_documents)\n",
    "        # print(\"Iinferring vectors...\")\n",
    "        vectorized_texts = []\n",
    "        for doc_id, _ in enumerate(tqdm(X, desc=\"Inferring vectors: \")):\n",
    "            inferred_vector = self.model.infer_vector(X[doc_id].words)\n",
    "            vectorized_texts.append(inferred_vector)\n",
    "\n",
    "        return vectorized_texts\n",
    "\n",
    "    def preprocess(self, raw_documents):\n",
    "        # print(\"Tokenization...\")\n",
    "        processed_texts = []\n",
    "        for idx, text in enumerate(tqdm(raw_documents, desc=\"Tokenization: \")):\n",
    "            processed_texts.append(doc2vec.TaggedDocument(simple_preprocess(text), [idx]))\n",
    "        return processed_texts\n",
    "\n",
    "\n",
    "    def fit_transform(self, texts, y=None) -> np.ndarray:\n",
    "        self.fit(texts)\n",
    "        X = self.transform(texts)\n",
    "        \n",
    "        return np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение модели "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, которые я буду рассматривать:\n",
    "+ LogisticRegression\n",
    "+ GradientBoosting\n",
    "+ LinearSVC\n",
    "+ BayesianClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[\"text\"]\n",
    "# y = data.label.map({\"pos\": 1, \"neg\": 0})\n",
    "y = train_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "_model = make_pipeline(DummyPreprocessor(), CountVectorizer(ngram_range=(1,3)), LinearSVC(max_iter=4000, class_weight=\"balanced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5286,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dummypreprocessor',\n",
       "                 <__main__.DummyPreprocessor object at 0x7fe3f927abb0>),\n",
       "                ('countvectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('linearsvc',\n",
       "                 LinearSVC(C=1.0, class_weight='balanced', dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=4000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "execution_count": 5286
    }
   ],
   "source": [
    "_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.87357227, 0.86485422, 0.88687426, 0.85179346])"
      ]
     },
     "metadata": {},
     "execution_count": 5287
    }
   ],
   "source": [
    "cross_val_score(_model, X, y, n_jobs=6, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Улучшение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшить модель можно несколькими путями:\n",
    "+ 1. Попробовать разный препроцессинг текста\n",
    "+ 2. Подобрать параметры модели векторизации\n",
    "+ 3. Подобрать параметры классификатора\n",
    "+ 4. Попробовать другие классификаторы\n",
    "+ 5. Сбалансировать классы для обучения\n",
    "+ 0. Делать перебор не по сетке, а более \"разумными\" методами\n",
    "\n",
    "Я сделаю только пп. 1, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5293,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.21.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5289,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name '_yields_constant_splits' from 'sklearn.model_selection._split' (/home/master/.local/share/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5289-b0b9fdca6f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menable_halving_search_cv\u001b[0m \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHalvingRandomSearchCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/miniconda3/lib/python3.8/site-packages/sklearn/experimental/enable_halving_search_cv.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \"\"\"\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from ..model_selection._search_successive_halving import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mHalvingRandomSearchCV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mHalvingGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_search_successive_halving.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_yields_constant_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_yields_constant_splits' from 'sklearn.model_selection._split' (/home/master/.local/share/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint\n",
    "\n",
    "pipeline_ = Pipeline([\n",
    "    (\"preprocessor\", RoughPreprocessor()),\n",
    "    (\"vec\", GensimVectorizer()),\n",
    "    (\"clf\", GradientBoostingClassifier())\n",
    "])\n",
    "# я разбил на несколько словарей потому что \n",
    "# параметры не унифицированы между классами моделей\n",
    "linear_classifiers = {\n",
    "    \"clf\":[LogisticRegression(), LinearSVC()],\n",
    "    \"clf__class_weight\":[\"balanced\"],\n",
    "    \"clf__dual\": [True, False]\n",
    "}\n",
    "bayesian_classifiers = {\n",
    "    \"clf\": [GaussianNB()],\n",
    "}\n",
    "forest_classifiers = {\n",
    "    \"clf\":[GradientBoostingClassifier(), RandomForestClassifier()],\n",
    "    \"clf__n_estimators\":randint(100, 500),\n",
    "    \"clf__max_depth\": randint(3, 20),\n",
    "}\n",
    "preprocessors = {\n",
    "    \"preprocessor\":[RoughPreprocessor(), DummyPreprocessor()],\n",
    "}\n",
    "common_vectorizers = {\n",
    "    \"vec\": [CountVectorizer(), TfidfVectorizer()],\n",
    "    \"vec__ngram_range\": [(1,1), (1,2), (1,3), (1,4), (1,5)],\n",
    "    \"vec__min_df\": randint(1, 10),\n",
    "    \"vec__max_features\": [None, 200, 500, 1000, 350, 1500]\n",
    "}\n",
    "embedding_vectorizers = {\n",
    "    \"vec\": [GensimVectorizer()]\n",
    "}\n",
    "\n",
    "param_distributions = [\n",
    "{\n",
    "    # Plain Vectorizers + Linear Models\n",
    "    **preprocessors,\n",
    "    **common_vectorizers,\n",
    "    **linear_classifiers    \n",
    "}, \n",
    "# {\n",
    "#     # Plain Vectorizers + Linear Models\n",
    "#     **preprocessors,\n",
    "#     **common_vectorizers,\n",
    "#     **bayesian_classifiers    \n",
    "# },\n",
    "# {\n",
    "#     # Gensim Embeddings + Linear Models\n",
    "#     **preprocessors,\n",
    "#     **embedding_vectorizers,\n",
    "#     **linear_classifiers,\n",
    "# }, \n",
    "# {\n",
    "#     # Gensim Embeddings + GradBoost Models\n",
    "#     **preprocessors,\n",
    "#     **embedding_vectorizers,\n",
    "#     **forest_classifiers,\n",
    "# }, \n",
    "# {\n",
    "#     # Plain Vectorizers + GradBoost Models\n",
    "#     **preprocessors,\n",
    "#     **common_vectorizers,\n",
    "#     **forest_classifiers,\n",
    "# }\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4008,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 2400 candidates, totalling 7200 fits\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=6)]: Done 353 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=6)]: Done 636 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=6)]: Done 1001 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=6)]: Done 1446 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=6)]: Done 1973 tasks      | elapsed: 29.9min\n",
      "[Parallel(n_jobs=6)]: Done 2580 tasks      | elapsed: 40.5min\n",
      "[Parallel(n_jobs=6)]: Done 3269 tasks      | elapsed: 48.8min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed: 60.3min\n",
      "[Parallel(n_jobs=6)]: Done 4889 tasks      | elapsed: 74.9min\n",
      "[Parallel(n_jobs=6)]: Done 5820 tasks      | elapsed: 91.0min\n",
      "[Parallel(n_jobs=6)]: Done 6833 tasks      | elapsed: 109.5min\n",
      "[Parallel(n_jobs=6)]: Done 7200 out of 7200 | elapsed: 120.3min finished\n",
      "CPU times: user 1h 41min 7s, sys: 3min 39s, total: 1h 44min 46s\n",
      "Wall time: 2h 17s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        <__main__.RoughPreprocessor object at 0x7fe3c2f1e0d0>),\n",
       "                                       ('vec', GensimVectorizer()),\n",
       "                                       ('clf',\n",
       "                                        GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                                   init=None,\n",
       "                                                                   learning_rate=0.1,\n",
       "                                                                   loss='deviance',\n",
       "                                                                   max_depth=3,\n",
       "                                                                   max_features=None,\n",
       "                                                                   max_leaf_nodes=None,\n",
       "                                                                   min_impurity_d...\n",
       "                                                  stop_words=None,\n",
       "                                                  strip_accents=None,\n",
       "                                                  sublinear_tf=False,\n",
       "                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                  tokenizer=None, use_idf=True,\n",
       "                                                  vocabulary=None)],\n",
       "                          'vec__max_features': [None, 200, 500, 1000, 350,\n",
       "                                                1500],\n",
       "                          'vec__min_df': [1, 2, 3, 4, 10],\n",
       "                          'vec__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4),\n",
       "                                               (1, 5)]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 4008
    }
   ],
   "source": [
    "%%time\n",
    "# ~ несколько недель на 6 ядрах Ryzen5\n",
    "hrscv_ = HalvingRandomSearchCV(\n",
    "    estimator=pipeline_,\n",
    "    param_distributions=param_distributions, \n",
    "    scoring=\"f1\", \n",
    "    n_jobs=6, refit=True, cv=3, verbose=2)\n",
    "hrscv_.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший набор параметров:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4009,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8874002561324007\n{'clf': LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n          verbose=0), 'clf__class_weight': 'balanced', 'clf__dual': True, 'preprocessor': <__main__.RoughPreprocessor object at 0x7fe3c2f1e1f0>, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\n                input='content', lowercase=True, max_df=1.0, max_features=None,\n                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n                smooth_idf=True, stop_words=None, strip_accents=None,\n                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, use_idf=True, vocabulary=None), 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(gscv_.best_score_)\n",
    "print(gscv_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9819234084472356\n",
    "{'clf': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "                           learning_rate=0.1, loss='deviance', max_depth=10,\n",
    "                           max_features=None, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "                           n_iter_no_change=None, presort='auto',\n",
    "                           random_state=None, subsample=1.0, tol=0.0001,\n",
    "                           validation_fraction=0.1, verbose=0,\n",
    "                           warm_start=False), 'clf__max_depth': 10, 'clf__n_estimators': 500, 'preprocessor': <__main__.DummyPreprocessor object at 0x7fe9618d6d30>, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
    "                lowercase=True, max_df=1.0, max_features=1000, min_df=2,\n",
    "                ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
    "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                tokenizer=None, vocabulary=None), 'vec__max_features': 1000, 'vec__min_df': 2, 'vec__ngram_range': (1, 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4010,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = gscv_.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3063,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = _model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Инференс и подготовка сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это для импорта предоставленного файла\n",
    "import bs4\n",
    "test = []\n",
    "with open(\"test.csv\") as tfile:\n",
    "    sp = bs4.BeautifulSoup(tfile)\n",
    "    revs = sp.findAll(\"review\")\n",
    "    for r in revs:\n",
    "        test.append(r.text)\n",
    "\n",
    "# pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4019,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а это мой размеченный тестовый файл\n",
    "test = pd.read_csv(\"./test_.csv\", usecols=[\"text\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4020,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  y\n",
       "0  Ужасно слабый аккумулятор, это основной минус ...  0\n",
       "1  ценанадежность-неубиваемостьдолго держит батар...  1\n",
       "2  подробнее в комментариях\\nК сожалению, факт по...  0\n",
       "3  я любительница громкой музыки. Тише телефона у...  0\n",
       "4  Дата выпуска - 2011 г, емкость - 1430 mAh, тех...  1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ужасно слабый аккумулятор, это основной минус ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ценанадежность-неубиваемостьдолго держит батар...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>подробнее в комментариях\\nК сожалению, факт по...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>я любительница громкой музыки. Тише телефона у...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Дата выпуска - 2011 г, емкость - 1430 mAh, тех...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4020
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4011,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"prediction\"] = model_final.predict(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4012,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "metadata": {},
     "execution_count": 4012
    }
   ],
   "source": [
    "accuracy_score(test[\"y\"], test[\"prediction\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# моя разметка тестового файла не очень точная.\n",
    "accuracy_score(test[\"y\"], test[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.copy()\n",
    "submission[\"y\"] = test.prediction.map({1: \"pos\", 0: \"neg\"})\n",
    "submission.to_csv(\"./submission.csv\", columns=[\"y\"], index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Упаковка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В финальный пайп я пакую предобученную на большой выборке модель векторизации \n",
    "# и классификатор, обученный на выборке после ресемплинга\n",
    "final_pipeline = model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../SentimentModelRU.pkl\", \"wb\") as fout:\n",
    "    dill.dump(final_pipeline, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../SentimentModelRU.pkl\", \"rb\") as fin:\n",
    "    v = dill.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with openwith open(\"../SentimentModelRU.pkl\", \"wb\") as fout:\n",
    "    dill.dump(final_pipeline, fout)with open(\"../SentimentModelRU.pkl\", \"wb\") as fout:\n",
    "    dill.dump(final_pipeline, fout)(\"../SentimentModelRU.pkl\", \"wb\") as fout:\n",
    "    dill.dump(final_pipeline, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}