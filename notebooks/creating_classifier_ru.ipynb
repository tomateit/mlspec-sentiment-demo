{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отзывы для обучения уже загружены скриптом в SQLite3 бд.\n",
    "\n",
    "Часть данных была вручную размечена для улучшения качества. Эти датасеты не включены в репозиторий, однако я готов ими поделиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import html\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label  assessed\n",
       "0  Очень хороший телефон, первый из линейки Redmi...      1         0\n",
       "1  Одним словом бонба, айфон курит в сторонке. Хо...      1         0\n",
       "2                                           Доволен.      1         0\n",
       "3                                       превосходно.      1         1\n",
       "4  Телефон покупал как замену Редми 5+ всем устра...      1         1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>assessed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Очень хороший телефон, первый из линейки Redmi...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Одним словом бонба, айфон курит в сторонке. Хо...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Доволен.</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>превосходно.</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Телефон покупал как замену Редми 5+ всем устра...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"./train_data_other.csv\", usecols=[\"text\", \"label\", \"assessed\"])\n",
    "# train_data[\"label\"] = train_data[\"label\"].map({\"pos\":1,\"neg\":0})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Плюсы: Хорошая камера, получаются четкие снимки в режиме 64мп Производительность Красивый Хороший экран Держит заряд целый день и еще остаётся процентов 30, если не играть в игры Соотношение цена/качество Присутствует модуль NFS . Минусы: Сканер отпечатка пальцев находиться рядом с камерами Вырез капелька Маркий и очень скользкий Убрали возможность записывать звонки, но это не только у этой версии телефона. Впечатления: Очень хороший телефон, первый из линейки Redmi у которого есть NFC, многие бояться процессора MediaTek, но телефон очень производительный, тянет игры на максималках и не сильно греется. Камеры бомба, присутствует даже макрообъектив. Без чехла лучше не носить, скользкий и легко уронить. В общем достойная модель, учитывая что на алике можно заказать за 14к .'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_data.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Подготовка текста"
   ]
  },
  {
   "source": [
    "Попробую аугментировать мой набор данных. Специфика такая, что в отзыв могут написать как хорошее, так и плохое (для этого есть отдельные секции). Две идеи\n",
    "\n",
    "1. Добавить отзывы, состоящие только из содержимого секций \"Плюсы\" и \"Минусы\" с соответствующими лейблами\n",
    "2. Добавить отзывы, в которых, в зависимости от известного лейбла, убрано содержимое противоположной секции"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_chunks = []\n",
    "for _, row in train_data.iterrows():\n",
    "    try:\n",
    "        _positive, _remaining = row.text.split(\"Минусы: \")\n",
    "        _negative, _remaining = _remaining.split(\"Впечатления: \")\n",
    "        _positive = _positive.replace(\"Плюсы: \", \"\")\n",
    "        separated_chunks.append({\"positive\": _positive, \"negative\": _negative, \"other\": _remaining, \"label\": row.label})\n",
    "    except ValueError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10151"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(separated_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'positive': '1) Экран. 2) Корпус из металла. ',\n",
       " 'negative': '1) Самый жирный - система IOS (как в тюрьме или в армии) 2) Цена высокая 3) Скользкий 4) За такую цену качество должно превосходить все смартфоны, но увы - наоборот. . ',\n",
       " 'other': 'Впечатлений масса, но все они не в пользу этого смартфона (ипхона).',\n",
       " 'label': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "separated_chunks[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data_synthetic_labels = []\n",
    "for chunk in separated_chunks:\n",
    "    aug_data_synthetic_labels.append({\"text\": chunk[\"positive\"], \"label\": 1})\n",
    "    aug_data_synthetic_labels.append({\"text\": chunk[\"negative\"], \"label\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data_irrelevant_separated = []\n",
    "for chunk in separated_chunks:\n",
    "    aug_data_synthetic_labels.append({\n",
    "        \"text\": chunk[\"positive\"] + \" \" + chunk[\"other\"] if chunk[\"label\"] == 1 else chunk[\"negative\"] + \" \" + chunk[\"other\"], \n",
    "        \"label\": chunk[\"label\"]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data_feedback_only = []\n",
    "for chunk in separated_chunks:\n",
    "    aug_data_feedback_only.append({\n",
    "        \"text\": chunk[\"other\"], \n",
    "        \"label\": chunk[\"label\"]\n",
    "        })\n",
    "train_data = pd.DataFrame(aug_data_feedback_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.append(pd.DataFrame(aug_data_synthetic_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.append(pd.DataFrame(aug_data_irrelevant_separated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "\n",
    "for (idx, row) in train_data[train_data.assessed == 0].iterrows():\n",
    "    print(f\"проверено: {sum(train_data.assessed)}    осталось {len(train_data) - sum(train_data.assessed)}\")\n",
    "    print(f\"idx: {idx}\")\n",
    "    print(row.label)\n",
    "    print(row.text)\n",
    "    train_data.at[idx, 'assessed'] = 1\n",
    "    break\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4048,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "проверено: 7751    осталось 2400\nidx: 6029\n1\nОтличная модель за приемлемую цену..\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4002,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.at[idx, 'label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4020,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.at[idx, 'label'] = 1"
   ]
  },
  {
   "source": [
    "idx = randomset[-2]\n",
    "print(train_data.iloc[_rn].label)\n",
    "print(train_data.iloc[_rn].text)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5056,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\nВ эксплуатации почти 1 год. Аппарат не плохой. Работает стабильно. Батарея держит хорошо. До этого сидел на яблоке. Если сравнивать, то система iOS это премиум, андроид это эконом. От яблока отказался - за долбали обновления и умышленные действия яблока направленные на покупку новых аппаратов . .\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4049,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_data_other.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2515,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "text        В целом модель то не плохая, но с каждым обнов...\n",
       "label                                                       0\n",
       "assessed                                                    1\n",
       "Name: 1261, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 2515
    }
   ],
   "source": [
    "train_data.iloc[1261]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что касается предобработки, то я буду использовать два варианта: довольно стерильные наборы слов и +- оригинальные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2516,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "class RoughPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self, data, y=None):\n",
    "        return list(map(self.normalize_text_re, map(self.normalize_text, data)))\n",
    "    \n",
    "    def normalize_text(self, text):\n",
    "        _t = html.unescape(text)\n",
    "        _t = _t.replace(\"Плюсы: \",\". \")\n",
    "        _t = _t.replace(\"Минусы: \",\". \")\n",
    "        _t = _t.replace(\"Впечатления: \",\". \")\n",
    "        _t = _t.replace(\"<p>\",\" \")\n",
    "        _t = _t.replace(\"</p>\", \" \")\n",
    "        _t = _t.replace(\"\\n\", \" \")\n",
    "        _t = _t.replace(\"\\r\", \" \")\n",
    "        _t = _t.replace(\"\\t\", \" \")\n",
    "        _t = _t.replace('\"', \" \")\n",
    "        _t = _t.lower()\n",
    "        return _t.strip()\n",
    "    \n",
    "    def normalize_text_re(self, text):\n",
    "        _t = text\n",
    "        _t = re.sub(r\"[\\s.,\\-\\+><;:!?()]\", \" \", _t)\n",
    "        _t = re.sub(r\"\\s+\", \" \", _t)\n",
    "        return _t.strip()\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return self.fit_transform(data)\n",
    "    \n",
    "    def transform(self, data, y=None):\n",
    "        return self.fit_transform(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy препроцессор, чтоб проще пайплайн строить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self, data, y=None):\n",
    "        return data\n",
    "    \n",
    "    def transform(self, data, y=None):\n",
    "        return data\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я хочу воспользоваться библиотекой gensim, потому что я уже использовал их эмбеддинги, и они отлично показали себя в классификации, даже на плохо обработанном датасете.\n",
    "\n",
    "Чтобы потом удобно запаковать это в sklearn pipeline, я реализую свой класс по образу `TfIdfVectorizer` из `sklearn.feature_extraction`\n",
    "\n",
    "Также я попробую TfIdfVectorizer и CountVectorizer сам по себе для сравнения какой лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2519,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import doc2vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "\n",
    "class GensimVectorizer(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        X = self.preprocess(raw_documents)\n",
    "        # print(\"Creating model...\")\n",
    "        model = gensim.models.doc2vec.Doc2Vec(\n",
    "            vector_size=100, \n",
    "            min_count=10,\n",
    "            epochs=40\n",
    "        )\n",
    "        # print(\"Building vocab...\")\n",
    "        model.build_vocab(X)\n",
    "        # print(\"Training doc2vec...\")\n",
    "        model.train(X, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_documents, y=None):\n",
    "        X = self.preprocess(raw_documents)\n",
    "        # print(\"Iinferring vectors...\")\n",
    "        vectorized_texts = []\n",
    "        for doc_id, _ in enumerate(tqdm(X, desc=\"Inferring vectors: \")):\n",
    "            inferred_vector = self.model.infer_vector(X[doc_id].words)\n",
    "            vectorized_texts.append(inferred_vector)\n",
    "\n",
    "        return vectorized_texts\n",
    "\n",
    "    def preprocess(self, raw_documents):\n",
    "        # print(\"Tokenization...\")\n",
    "        processed_texts = []\n",
    "        for idx, text in enumerate(tqdm(raw_documents, desc=\"Tokenization: \")):\n",
    "            processed_texts.append(doc2vec.TaggedDocument(simple_preprocess(text), [idx]))\n",
    "        return processed_texts\n",
    "\n",
    "\n",
    "    def fit_transform(self, texts, y=None) -> np.ndarray:\n",
    "        self.fit(texts)\n",
    "        X = self.transform(texts)\n",
    "        \n",
    "        return np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение модели "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, которые я буду рассматривать:\n",
    "+ LogisticRegression\n",
    "+ GradientBoosting\n",
    "+ LinearSVC\n",
    "+ BayesianClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4050,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[\"text\"]\n",
    "# y = data.label.map({\"pos\": 1, \"neg\": 0})\n",
    "y = train_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2522,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "_model = make_pipeline(DummyPreprocessor(), CountVectorizer(ngram_range=(1,3)), LinearSVC(max_iter=4000, class_weight=\"balanced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4051,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('dummypreprocessor',\n",
       "                 <__main__.DummyPreprocessor object at 0x7ff63a088ac0>),\n",
       "                ('countvectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
       "                ('linearsvc',\n",
       "                 LinearSVC(class_weight='balanced', max_iter=4000))])"
      ]
     },
     "metadata": {},
     "execution_count": 4051
    }
   ],
   "source": [
    "_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4052,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.9034673 , 0.90464933, 0.90031521, 0.86677178])"
      ]
     },
     "metadata": {},
     "execution_count": 4052
    }
   ],
   "source": [
    "cross_val_score(_model, X, y, n_jobs=6, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Улучшение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшить модель можно несколькими путями:\n",
    "+ 1. Попробовать разный препроцессинг текста\n",
    "+ 2. Подобрать параметры модели векторизации\n",
    "+ 3. Подобрать параметры классификатора\n",
    "+ 4. Попробовать другие классификаторы\n",
    "+ 5. Сбалансировать классы для обучения\n",
    "+ 0. Делать перебор не по сетке, а более \"разумными\" методами\n",
    "\n",
    "Я сделаю только пп. 1, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3232,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint\n",
    "\n",
    "pipeline_ = Pipeline([\n",
    "    (\"preprocessor\", RoughPreprocessor()),\n",
    "    (\"vec\", GensimVectorizer()),\n",
    "    (\"clf\", GradientBoostingClassifier())\n",
    "])\n",
    "# я разбил на несколько словарей потому что \n",
    "# параметры не унифицированы между классами моделей\n",
    "linear_classifiers = {\n",
    "    \"clf\":[LogisticRegression(), LinearSVC()],\n",
    "    \"clf__class_weight\":[\"balanced\"],\n",
    "    # \"clf__dual\": [True, False]\n",
    "}\n",
    "bayesian_classifiers = {\n",
    "    \"clf\": [GaussianNB()],\n",
    "}\n",
    "forest_classifiers = {\n",
    "    \"clf\":[GradientBoostingClassifier(), RandomForestClassifier()],\n",
    "    \"clf__n_estimators\":randint(100, 500),\n",
    "    \"clf__max_depth\": randint(3, 20),\n",
    "}\n",
    "preprocessors = {\n",
    "    \"preprocessor\":[RoughPreprocessor(), DummyPreprocessor()],\n",
    "}\n",
    "common_vectorizers = {\n",
    "    \"vec\": [CountVectorizer(), TfidfVectorizer()],\n",
    "    \"vec__ngram_range\": [(1,1), (1,2), (1,3), (1,4), (1,5)],\n",
    "    \"vec__min_df\": randint(1, 10),\n",
    "    \"vec__max_features\": [None, 200, 500, 1000, 350, 1500]\n",
    "}\n",
    "embedding_vectorizers = {\n",
    "    \"vec\": [GensimVectorizer()]\n",
    "}\n",
    "\n",
    "param_distributions = [\n",
    "{\n",
    "    # Plain Vectorizers + Linear Models\n",
    "    **preprocessors,\n",
    "    **common_vectorizers,\n",
    "    **linear_classifiers    \n",
    "}, \n",
    "# {\n",
    "#     # Plain Vectorizers + Linear Models\n",
    "#     **preprocessors,\n",
    "#     **common_vectorizers,\n",
    "#     **bayesian_classifiers    \n",
    "# },\n",
    "# {\n",
    "#     # Gensim Embeddings + Linear Models\n",
    "#     **preprocessors,\n",
    "#     **embedding_vectorizers,\n",
    "#     **linear_classifiers,\n",
    "# }, \n",
    "# {\n",
    "#     # Gensim Embeddings + GradBoost Models\n",
    "#     **preprocessors,\n",
    "#     **embedding_vectorizers,\n",
    "#     **forest_classifiers,\n",
    "# }, \n",
    "# {\n",
    "#     # Plain Vectorizers + GradBoost Models\n",
    "#     **preprocessors,\n",
    "#     **common_vectorizers,\n",
    "#     **forest_classifiers,\n",
    "# }\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4053,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 12\n",
      "max_resources_: 10151\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 845\n",
      "n_resources: 12\n",
      "Fitting 3 folds for each of 845 candidates, totalling 2535 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 282\n",
      "n_resources: 36\n",
      "Fitting 3 folds for each of 282 candidates, totalling 846 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 94\n",
      "n_resources: 108\n",
      "Fitting 3 folds for each of 94 candidates, totalling 282 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 32\n",
      "n_resources: 324\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 11\n",
      "n_resources: 972\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 4\n",
      "n_resources: 2916\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 2\n",
      "n_resources: 8748\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "CPU times: user 5.08 s, sys: 269 ms, total: 5.34 s\n",
      "Wall time: 13.4 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HalvingRandomSearchCV(cv=3,\n",
       "                      estimator=Pipeline(steps=[('preprocessor',\n",
       "                                                 <__main__.RoughPreprocessor object at 0x7ff639e73280>),\n",
       "                                                ('vec', GensimVectorizer()),\n",
       "                                                ('clf',\n",
       "                                                 GradientBoostingClassifier())]),\n",
       "                      n_jobs=6,\n",
       "                      param_distributions=[{'clf': [LogisticRegression(class_weight='balanced'),\n",
       "                                                    LinearSVC(class_weight='balanced')],\n",
       "                                            'clf__class_weight': ['balanced'],\n",
       "                                            'preprocesso...\n",
       "                                            'vec': [CountVectorizer(min_df=7,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 4)),\n",
       "                                                    TfidfVectorizer(max_features=1500,\n",
       "                                                                    min_df=9,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 3))],\n",
       "                                            'vec__max_features': [None, 200,\n",
       "                                                                  500, 1000,\n",
       "                                                                  350, 1500],\n",
       "                                            'vec__min_df': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ff639e5a880>,\n",
       "                                            'vec__ngram_range': [(1, 1), (1, 2),\n",
       "                                                                 (1, 3), (1, 4),\n",
       "                                                                 (1, 5)]}],\n",
       "                      refit=<function _refit_callable at 0x7ff645672ee0>,\n",
       "                      scoring='f1', verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 4053
    }
   ],
   "source": [
    "%%time\n",
    "# ~ несколько недель на 6 ядрах Ryzen5\n",
    "hrscv_ = HalvingRandomSearchCV(\n",
    "    estimator=pipeline_,\n",
    "    param_distributions=param_distributions, \n",
    "    scoring=\"f1\", \n",
    "    n_jobs=6, refit=True, cv=3, verbose=2)\n",
    "hrscv_.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший набор параметров:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4054,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8852723710256737\n{'clf': LinearSVC(class_weight='balanced'), 'clf__class_weight': 'balanced', 'preprocessor': <__main__.RoughPreprocessor object at 0x7ff639e73d90>, 'vec': TfidfVectorizer(max_features=1500, min_df=9, ngram_range=(1, 3)), 'vec__max_features': 1500, 'vec__min_df': 9, 'vec__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(hrscv_.best_score_)\n",
    "print(hrscv_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9819234084472356\n",
    "{'clf': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "                           learning_rate=0.1, loss='deviance', max_depth=10,\n",
    "                           max_features=None, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "                           n_iter_no_change=None, presort='auto',\n",
    "                           random_state=None, subsample=1.0, tol=0.0001,\n",
    "                           validation_fraction=0.1, verbose=0,\n",
    "                           warm_start=False), 'clf__max_depth': 10, 'clf__n_estimators': 500, 'preprocessor': <__main__.DummyPreprocessor object at 0x7fe9618d6d30>, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
    "                lowercase=True, max_df=1.0, max_features=1000, min_df=2,\n",
    "                ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
    "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                tokenizer=None, vocabulary=None), 'vec__max_features': 1000, 'vec__min_df': 2, 'vec__ngram_range': (1, 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4066,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelne_final = hrscv_.best_estimator_\n",
    "classifier_final = hrscv_.best_estimator_[\"clf\"]\n",
    "vectorizer_final = hrscv_.best_estimator_[\"vec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4064,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<1x1500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 4064
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = _model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Инференс и подготовка сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это для импорта предоставленного файла\n",
    "import bs4\n",
    "test = []\n",
    "with open(\"test.csv\") as tfile:\n",
    "    sp = bs4.BeautifulSoup(tfile)\n",
    "    revs = sp.findAll(\"review\")\n",
    "    for r in revs:\n",
    "        test.append(r.text)\n",
    "\n",
    "# pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а это мой размеченный тестовый файл\n",
    "test = pd.read_csv(\"./test_.csv\", usecols=[\"text\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  y\n",
       "0  Ужасно слабый аккумулятор, это основной минус ...  0\n",
       "1  ценанадежность-неубиваемостьдолго держит батар...  1\n",
       "2  подробнее в комментариях\\nК сожалению, факт по...  0\n",
       "3  я любительница громкой музыки. Тише телефона у...  0\n",
       "4  Дата выпуска - 2011 г, емкость - 1430 mAh, тех...  1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ужасно слабый аккумулятор, это основной минус ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ценанадежность-неубиваемостьдолго держит батар...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>подробнее в комментариях\\nК сожалению, факт по...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>я любительница громкой музыки. Тише телефона у...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Дата выпуска - 2011 г, емкость - 1430 mAh, тех...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 488
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4056,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"prediction\"] = model_final.predict(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4057,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "metadata": {},
     "execution_count": 4057
    }
   ],
   "source": [
    "accuracy_score(test[\"y\"], test[\"prediction\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# моя разметка тестового файла не очень точная.\n",
    "accuracy_score(test[\"y\"], test[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.copy()\n",
    "submission[\"y\"] = test.prediction.map({1: \"pos\", 0: \"neg\"})\n",
    "submission.to_csv(\"./submission.csv\", columns=[\"y\"], index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Упаковка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import dill\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4068,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/SentimentModelRU.pkl\", \"wb\") as fout:\n",
    "    dill.dump(classifier_final, fout)\n",
    "with open(\"../models/VectorizerRU.pkl\", \"wb\") as fout:\n",
    "    dill.dump(vectorizer_final, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4069,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "file must have 'read' and 'readline' attributes",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4069-eda82d00d22a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/SentimentModelRU.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclassifier_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/VectorizerRU.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvectorizer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, ignore, **kwds)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;34m\"\"\"unpickle an object from a file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0m_ignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0mStockUnpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_main_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_ignore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have 'read' and 'readline' attributes"
     ]
    }
   ],
   "source": [
    "with open(\"../models/SentimentModelRU.pkl\", \"rb\") as fin:\n",
    "    classifier_ = dill.load(fin)\n",
    "with open(\"../models/VectorizerRU.pkl\", \"rb\") as fin:\n",
    "    vectorizer_ = dill.dump(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3243,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['../SentimentModelRU.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 3243
    }
   ],
   "source": [
    "joblib.dump(final_pipeline, \"../SentimentModelRU.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}