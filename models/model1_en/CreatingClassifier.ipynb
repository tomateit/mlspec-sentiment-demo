{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating model for sentiment classification with scikit-learn (EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data retrieval\n",
    "\n",
    "For this model, movie revies dataset will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pathlib import Path\n",
    "nltk.data.path.append(f\"{str(Path.home())}/datasets\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "negfeats = [movie_reviews.words(fileids=[f]) for f in negids]\n",
    "posfeats = [movie_reviews.words(fileids=[f]) for f in posids]\n",
    "X = [*negfeats, *posfeats]\n",
    "X = [\" \".join(list(x)) for x in X]\n",
    "y = [0]*len(negfeats)+[1]*len(posfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "sum(y)/len(y) # classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classical approach\n",
    "This will include sklearn toolset to vectorize data and build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    TfidfVectorizer(stop_words=nltk.corpus.stopwords.words('english')), \n",
    "    GradientBoostingClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.79  , 0.8125, 0.775 , 0.7875, 0.8075])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint\n",
    "\n",
    "pipeline_ = Pipeline([\n",
    "    (\"vec\", TfidfVectorizer()),\n",
    "    (\"clf\", GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "vectorizers = {\n",
    "    \"vec\": [CountVectorizer(), TfidfVectorizer()],\n",
    "    \"vec__ngram_range\": [(1,1), (1,2), (1,3), (1,4), (1,5)],\n",
    "    \"vec__min_df\": randint(1, 10),\n",
    "    \"vec__max_features\": [None, 200, 500, 1000, 350, 1500]\n",
    "}\n",
    "classifiers = {\n",
    "    \"clf\":[GradientBoostingClassifier()],\n",
    "    \"clf__n_estimators\":randint(100, 500),\n",
    "    \"clf__max_depth\": randint(3, 20),\n",
    "}\n",
    "\n",
    "\n",
    "param_distributions = [{\n",
    "    **vectorizers,\n",
    "    **classifiers    \n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 20\n",
      "max_resources_: 2000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 100\n",
      "n_resources: 20\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 34\n",
      "n_resources: 60\n",
      "Fitting 5 folds for each of 34 candidates, totalling 170 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 12\n",
      "n_resources: 180\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 4\n",
      "n_resources: 540\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 2\n",
      "n_resources: 1620\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "CPU times: user 50.1 s, sys: 1.34 s, total: 51.5 s\n",
      "Wall time: 4min 33s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HalvingRandomSearchCV(estimator=Pipeline(steps=[('vec', TfidfVectorizer()),\n",
       "                                                ('clf',\n",
       "                                                 GradientBoostingClassifier())]),\n",
       "                      n_jobs=6,\n",
       "                      param_distributions=[{'clf': [GradientBoostingClassifier(max_depth=4,\n",
       "                                                                               n_estimators=326)],\n",
       "                                            'clf__max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f2415e95130>,\n",
       "                                            'clf__n_estimators': <scipy.stats._distn_infrastructure.r...0a790>,\n",
       "                                            'vec': [CountVectorizer(),\n",
       "                                                    TfidfVectorizer(max_features=1000,\n",
       "                                                                    min_df=4,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 2))],\n",
       "                                            'vec__max_features': [None, 200,\n",
       "                                                                  500, 1000,\n",
       "                                                                  350, 1500],\n",
       "                                            'vec__min_df': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f241600a130>,\n",
       "                                            'vec__ngram_range': [(1, 1), (1, 2),\n",
       "                                                                 (1, 3), (1, 4),\n",
       "                                                                 (1, 5)]}],\n",
       "                      refit=<function _refit_callable at 0x7f241673e820>,\n",
       "                      scoring='f1', verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "%%time\n",
    "# ~ несколько недель на 6 ядрах Ryzen5\n",
    "hrscv_ = HalvingRandomSearchCV(\n",
    "    estimator=pipeline_,\n",
    "    param_distributions=param_distributions, \n",
    "    scoring=\"f1\", \n",
    "    n_jobs=6, refit=True, cv=5, verbose=2)\n",
    "hrscv_.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7808407857091266\n{'clf': GradientBoostingClassifier(max_depth=4, n_estimators=326), 'clf__max_depth': 4, 'clf__n_estimators': 326, 'vec': TfidfVectorizer(max_features=1000, min_df=4, ngram_range=(1, 2)), 'vec__max_features': 1000, 'vec__min_df': 4, 'vec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(hrscv_.best_score_)\n",
    "print(hrscv_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_final = hrscv_.best_estimator_[\"clf\"]\n",
    "vectorizer_final = hrscv_.best_estimator_[\"vec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sklearn model it is usually OK to use `dill` or `pickle`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_path = \"./Classifier.pkl\"\n",
    "vectorizer_path = \"./Vectorizer.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(classifier_path, \"wb\") as fout:\n",
    "    dill.dump(classifier_final, fout)\n",
    "with open(vectorizer_path, \"wb\") as fout:\n",
    "    dill.dump(vectorizer_final, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "classifier_final.predict(vectorizer_final.transform([\"excellent\"]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}